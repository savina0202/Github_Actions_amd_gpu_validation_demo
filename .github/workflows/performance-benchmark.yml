name: Performance Benchmark (Reusable)

on:
  workflow_call:

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-3.10-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-3.10-
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt pytest-benchmark
      
      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      
      - name: Run performance benchmarks
        run: |
          pytest -m performance --benchmark-only --benchmark-json=benchmark-results.json -v
        continue-on-error: true
      
      - name: Generate performance report
        run: |
          echo "Performance Benchmark Results" > perf-report.txt
          echo "=============================" >> perf-report.txt
          date >> perf-report.txt
          echo "" >> perf-report.txt
          if [ -f benchmark-results.json ]; then
            echo "Benchmark results generated successfully." >> perf-report.txt
          else
            echo "No benchmark results generated." >> perf-report.txt
          fi
        continue-on-error: true
      
      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-benchmark-reports
          path: |
            benchmark-results.json
            perf-report.txt
          retention-days: 30

